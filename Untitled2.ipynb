{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd220f94-a913-4aa9-8f61-e4557a3173c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_size = 88\n",
    "seq_len = 8\n",
    "d_model = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d05b0c-5c87-4d41-939b-ac0feb15806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random token embeddings: (vocab_size, d_model)\n",
    "token_embedding = np.random.randn(vocab_size, d_model) * 0.01\n",
    "\n",
    "# sinusoidal positional encoding: (seq_len, d_model)\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    # apply sin to even indices, cos to odd indices\n",
    "    pos_encoding = np.zeros((seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return pos_encoding\n",
    "\n",
    "positional_encoding = get_positional_encoding(seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc14e8c1-50bf-476d-8fa8-cfedd3944814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize projection matrices\n",
    "W_q = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "W_k = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "W_v = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "W_o = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "\n",
    "def self_attention(x):\n",
    "    # x shape: (seq_len, d_model)\n",
    "    Q = x @ W_q\n",
    "    K = x @ W_k\n",
    "    V = x @ W_v\n",
    "\n",
    "    scores = Q @ K.T / np.sqrt(d_model)         # (seq_len, seq_len)\n",
    "    mask = np.triu(np.ones_like(scores), 1) * -1e9  # causal mask\n",
    "    scores += mask\n",
    "\n",
    "    weights = softmax(scores)                   # (seq_len, seq_len)\n",
    "    return weights @ V @ W_o                    # (seq_len, d_model)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3bcaa06-0c36-4c6b-99cf-062b1c79b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layernorm parameters\n",
    "gamma = np.ones((d_model,))\n",
    "beta = np.zeros((d_model,))\n",
    "\n",
    "def layernorm(x, eps=1e-5):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    var = x.var(axis=-1, keepdims=True)\n",
    "    return gamma * (x - mean) / np.sqrt(var + eps) + beta\n",
    "\n",
    "# feedforward layer\n",
    "W1 = np.random.randn(d_model, d_model * 4) / np.sqrt(d_model)\n",
    "b1 = np.zeros((d_model * 4,))\n",
    "W2 = np.random.randn(d_model * 4, d_model) / np.sqrt(d_model * 4)\n",
    "b2 = np.zeros((d_model,))\n",
    "\n",
    "def feedforward(x):\n",
    "    return np.maximum(0, x @ W1 + b1) @ W2 + b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9030339-a051-4f39-ac8d-7904ec2b07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(x):\n",
    "    # input: (seq_len, d_model)\n",
    "    attn = self_attention(x)\n",
    "    x = layernorm(x + attn)       # residual + norm\n",
    "\n",
    "    ff = feedforward(x)\n",
    "    x = layernorm(x + ff)         # another residual + norm\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f58a70-9ff5-4ef1-ba7e-b293e2ac129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(tokens):\n",
    "    # tokens: list of ints, e.g. [60, 62, 64]\n",
    "    x = token_embedding[tokens] + positional_encoding[:len(tokens)]\n",
    "    x = transformer_block(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06521061-16d4-47e2-a1f5-185bfb7ba5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output head (shared with token_embedding is more efficient, but we keep it simple)\n",
    "W_out = np.random.randn(d_model, vocab_size) / np.sqrt(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1dcdbc9-6d63-4f17-9424-2ae81517300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: [60, 13, 8, 8, 67, 67, 67, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n"
     ]
    }
   ],
   "source": [
    "generated = generate_sequence([60], max_len=16)\n",
    "print(\"Generated sequence:\", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63a20e2c-82f8-4242-b3b6-2a300d3fd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_probs(probs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an index from a probability distribution with optional temperature.\n",
    "\n",
    "    Args:\n",
    "        probs: np.ndarray – shape (vocab_size,)\n",
    "        temperature: float – >1.0 = more random, <1.0 = more deterministic\n",
    "\n",
    "    Returns:\n",
    "        int – sampled index\n",
    "    \"\"\"\n",
    "    if temperature != 1.0:\n",
    "        probs = np.log(probs + 1e-9) / temperature\n",
    "        probs = np.exp(probs - np.max(probs))\n",
    "        probs = probs / np.sum(probs)\n",
    "    return int(np.random.choice(len(probs), p=probs))\n",
    "\n",
    "\n",
    "def predict_next_note(x, temperature=1.0):\n",
    "    logits = x[-1] @ W_out\n",
    "    probs = softmax(logits)\n",
    "    return sample_from_probs(probs, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb9aa824-2723-49cf-a30d-617fa72de512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(start_seq, max_len=32, temperature=1.0):\n",
    "    seq = list(start_seq)\n",
    "    while len(seq) < max_len:\n",
    "        input_tokens = seq[-seq_len:]\n",
    "        x = forward(input_tokens)\n",
    "        next_note = predict_next_note(x, temperature)\n",
    "        seq.append(next_note)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0474ebc-1554-4ace-b1aa-d5fcbe6904b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: [60, 80, 72, 5, 73, 0, 56, 37, 42, 44, 14, 69, 72, 69, 69, 84]\n"
     ]
    }
   ],
   "source": [
    "generated = generate_sequence([60], max_len=16, temperature=1)\n",
    "print(\"Generated:\", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db3f1955-77e5-475f-b360-31bd21ce6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_data():\n",
    "    data = []\n",
    "    for start in range(vocab_size - seq_len + 1):\n",
    "        seq = list(range(start, start + seq_len))\n",
    "        data.append(seq)\n",
    "    return np.array(data)\n",
    "\n",
    "def cross_entropy(pred, target_idx):\n",
    "    \"\"\"\n",
    "    pred: shape (vocab_size,) – probabilities\n",
    "    target_idx: int – ground-truth token\n",
    "    \"\"\"\n",
    "    return -np.log(pred[target_idx] + 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a96e8736-3b32-4fa3-acf5-90b0c4e3bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset everything\n",
    "token_embedding = np.random.randn(vocab_size, d_model) * 0.01\n",
    "W_out = np.random.randn(d_model, vocab_size) / np.sqrt(d_model)\n",
    "\n",
    "def forward_simple(tokens):\n",
    "    # mean embedding of the tokens\n",
    "    x = token_embedding[tokens].mean(axis=0)  # shape: (d_model,)\n",
    "    return x\n",
    "\n",
    "def train_simple(num_epochs=1000, lr=0.1):\n",
    "    data = make_training_data()\n",
    "    global token_embedding, W_out\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for seq in data:\n",
    "            x_tokens = seq[:-1]\n",
    "            y_target = seq[-1]\n",
    "\n",
    "            # forward\n",
    "            x = forward_simple(x_tokens)\n",
    "            logits = x @ W_out\n",
    "            probs = softmax(logits)\n",
    "            loss = cross_entropy(probs, y_target)\n",
    "            total_loss += loss\n",
    "\n",
    "            # backward\n",
    "            dlogits = probs\n",
    "            dlogits[y_target] -= 1  # ∇L\n",
    "\n",
    "            dW_out = np.outer(x, dlogits)\n",
    "            dx = dlogits @ W_out.T\n",
    "            d_embed = dx / len(x_tokens)  # distribute equally\n",
    "\n",
    "            # update\n",
    "            W_out -= lr * dW_out\n",
    "            for idx in x_tokens:\n",
    "                token_embedding[idx] -= lr * d_embed\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:03}: loss = {total_loss / len(data):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f0bfb30-33db-4da1-abc3-28da871cd1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: loss = 0.0009\n",
      "Epoch 011: loss = 0.0009\n",
      "Epoch 021: loss = 0.0009\n",
      "Epoch 031: loss = 0.0009\n",
      "Epoch 041: loss = 0.0009\n",
      "Epoch 051: loss = 0.0009\n",
      "Epoch 061: loss = 0.0009\n",
      "Epoch 071: loss = 0.0009\n",
      "Epoch 081: loss = 0.0009\n",
      "Epoch 091: loss = 0.0009\n",
      "Epoch 101: loss = 0.0009\n",
      "Epoch 111: loss = 0.0009\n",
      "Epoch 121: loss = 0.0009\n",
      "Epoch 131: loss = 0.0009\n",
      "Epoch 141: loss = 0.0009\n",
      "Epoch 151: loss = 0.0009\n",
      "Epoch 161: loss = 0.0009\n",
      "Epoch 171: loss = 0.0009\n",
      "Epoch 181: loss = 0.0009\n",
      "Epoch 191: loss = 0.0009\n",
      "Epoch 201: loss = 0.0009\n",
      "Epoch 211: loss = 0.0009\n",
      "Epoch 221: loss = 0.0009\n",
      "Epoch 231: loss = 0.0009\n",
      "Epoch 241: loss = 0.0009\n",
      "Epoch 251: loss = 0.0009\n",
      "Epoch 261: loss = 0.0009\n",
      "Epoch 271: loss = 0.0009\n",
      "Epoch 281: loss = 0.0009\n",
      "Epoch 291: loss = 0.0009\n",
      "Epoch 301: loss = 0.0009\n",
      "Epoch 311: loss = 0.0009\n",
      "Epoch 321: loss = 0.0009\n",
      "Epoch 331: loss = 0.0009\n",
      "Epoch 341: loss = 0.0009\n",
      "Epoch 351: loss = 0.0009\n",
      "Epoch 361: loss = 0.0009\n",
      "Epoch 371: loss = 0.0009\n",
      "Epoch 381: loss = 0.0009\n",
      "Epoch 391: loss = 0.0009\n",
      "Epoch 401: loss = 0.0009\n",
      "Epoch 411: loss = 0.0009\n",
      "Epoch 421: loss = 0.0009\n",
      "Epoch 431: loss = 0.0009\n",
      "Epoch 441: loss = 0.0009\n",
      "Epoch 451: loss = 0.0009\n",
      "Epoch 461: loss = 0.0009\n",
      "Epoch 471: loss = 0.0009\n",
      "Epoch 481: loss = 0.0009\n",
      "Epoch 491: loss = 0.0009\n",
      "Epoch 501: loss = 0.0009\n",
      "Epoch 511: loss = 0.0009\n",
      "Epoch 521: loss = 0.0009\n",
      "Epoch 531: loss = 0.0009\n",
      "Epoch 541: loss = 0.0009\n",
      "Epoch 551: loss = 0.0009\n",
      "Epoch 561: loss = 0.0009\n",
      "Epoch 571: loss = 0.0009\n",
      "Epoch 581: loss = 0.0009\n",
      "Epoch 591: loss = 0.0009\n",
      "Epoch 601: loss = 0.0009\n",
      "Epoch 611: loss = 0.0009\n",
      "Epoch 621: loss = 0.0009\n",
      "Epoch 631: loss = 0.0009\n",
      "Epoch 641: loss = 0.0009\n",
      "Epoch 651: loss = 0.0009\n",
      "Epoch 661: loss = 0.0009\n",
      "Epoch 671: loss = 0.0009\n",
      "Epoch 681: loss = 0.0009\n",
      "Epoch 691: loss = 0.0009\n",
      "Epoch 701: loss = 0.0009\n",
      "Epoch 711: loss = 0.0009\n",
      "Epoch 721: loss = 0.0009\n",
      "Epoch 731: loss = 0.0009\n",
      "Epoch 741: loss = 0.0009\n",
      "Epoch 751: loss = 0.0009\n",
      "Epoch 761: loss = 0.0009\n",
      "Epoch 771: loss = 0.0009\n",
      "Epoch 781: loss = 0.0009\n",
      "Epoch 791: loss = 0.0009\n",
      "Epoch 801: loss = 0.0009\n",
      "Epoch 811: loss = 0.0009\n",
      "Epoch 821: loss = 0.0009\n",
      "Epoch 831: loss = 0.0009\n",
      "Epoch 841: loss = 0.0009\n",
      "Epoch 851: loss = 0.0009\n",
      "Epoch 861: loss = 0.0009\n",
      "Epoch 871: loss = 0.0009\n",
      "Epoch 881: loss = 0.0009\n",
      "Epoch 891: loss = 0.0009\n",
      "Epoch 901: loss = 0.0009\n",
      "Epoch 911: loss = 0.0009\n",
      "Epoch 921: loss = 0.0009\n",
      "Epoch 931: loss = 0.0009\n",
      "Epoch 941: loss = 0.0009\n",
      "Epoch 951: loss = 0.0009\n",
      "Epoch 961: loss = 0.0009\n",
      "Epoch 971: loss = 0.0009\n",
      "Epoch 981: loss = 0.0009\n",
      "Epoch 991: loss = 0.0009\n",
      "Epoch 1001: loss = 0.0009\n",
      "Epoch 1011: loss = 0.0009\n",
      "Epoch 1021: loss = 0.0009\n",
      "Epoch 1031: loss = 0.0009\n",
      "Epoch 1041: loss = 0.0009\n",
      "Epoch 1051: loss = 0.0009\n",
      "Epoch 1061: loss = 0.0009\n",
      "Epoch 1071: loss = 0.0009\n",
      "Epoch 1081: loss = 0.0009\n",
      "Epoch 1091: loss = 0.0009\n",
      "Epoch 1101: loss = 0.0009\n",
      "Epoch 1111: loss = 0.0009\n",
      "Epoch 1121: loss = 0.0009\n",
      "Epoch 1131: loss = 0.0009\n",
      "Epoch 1141: loss = 0.0009\n",
      "Epoch 1151: loss = 0.0009\n",
      "Epoch 1161: loss = 0.0009\n",
      "Epoch 1171: loss = 0.0009\n",
      "Epoch 1181: loss = 0.0009\n",
      "Epoch 1191: loss = 0.0009\n",
      "Epoch 1201: loss = 0.0009\n",
      "Epoch 1211: loss = 0.0009\n",
      "Epoch 1221: loss = 0.0009\n",
      "Epoch 1231: loss = 0.0009\n",
      "Epoch 1241: loss = 0.0009\n",
      "Epoch 1251: loss = 0.0009\n",
      "Epoch 1261: loss = 0.0009\n",
      "Epoch 1271: loss = 0.0009\n",
      "Epoch 1281: loss = 0.0009\n",
      "Epoch 1291: loss = 0.0009\n",
      "Epoch 1301: loss = 0.0009\n",
      "Epoch 1311: loss = 0.0009\n",
      "Epoch 1321: loss = 0.0009\n",
      "Epoch 1331: loss = 0.0009\n",
      "Epoch 1341: loss = 0.0009\n",
      "Epoch 1351: loss = 0.0009\n",
      "Epoch 1361: loss = 0.0009\n",
      "Epoch 1371: loss = 0.0009\n",
      "Epoch 1381: loss = 0.0009\n",
      "Epoch 1391: loss = 0.0009\n",
      "Epoch 1401: loss = 0.0009\n",
      "Epoch 1411: loss = 0.0009\n",
      "Epoch 1421: loss = 0.0009\n",
      "Epoch 1431: loss = 0.0009\n",
      "Epoch 1441: loss = 0.0009\n",
      "Epoch 1451: loss = 0.0009\n",
      "Epoch 1461: loss = 0.0009\n",
      "Epoch 1471: loss = 0.0009\n",
      "Epoch 1481: loss = 0.0009\n",
      "Epoch 1491: loss = 0.0009\n",
      "Epoch 1501: loss = 0.0009\n",
      "Epoch 1511: loss = 0.0009\n",
      "Epoch 1521: loss = 0.0009\n",
      "Epoch 1531: loss = 0.0009\n",
      "Epoch 1541: loss = 0.0009\n",
      "Epoch 1551: loss = 0.0009\n",
      "Epoch 1561: loss = 0.0009\n",
      "Epoch 1571: loss = 0.0009\n",
      "Epoch 1581: loss = 0.0009\n",
      "Epoch 1591: loss = 0.0009\n",
      "Epoch 1601: loss = 0.0009\n",
      "Epoch 1611: loss = 0.0009\n",
      "Epoch 1621: loss = 0.0009\n",
      "Epoch 1631: loss = 0.0009\n",
      "Epoch 1641: loss = 0.0009\n",
      "Epoch 1651: loss = 0.0009\n",
      "Epoch 1661: loss = 0.0009\n",
      "Epoch 1671: loss = 0.0009\n",
      "Epoch 1681: loss = 0.0009\n",
      "Epoch 1691: loss = 0.0009\n",
      "Epoch 1701: loss = 0.0009\n",
      "Epoch 1711: loss = 0.0009\n",
      "Epoch 1721: loss = 0.0009\n",
      "Epoch 1731: loss = 0.0009\n",
      "Epoch 1741: loss = 0.0009\n",
      "Epoch 1751: loss = 0.0009\n",
      "Epoch 1761: loss = 0.0009\n",
      "Epoch 1771: loss = 0.0009\n",
      "Epoch 1781: loss = 0.0009\n",
      "Epoch 1791: loss = 0.0009\n",
      "Epoch 1801: loss = 0.0009\n",
      "Epoch 1811: loss = 0.0009\n",
      "Epoch 1821: loss = 0.0009\n",
      "Epoch 1831: loss = 0.0009\n",
      "Epoch 1841: loss = 0.0009\n",
      "Epoch 1851: loss = 0.0009\n",
      "Epoch 1861: loss = 0.0009\n",
      "Epoch 1871: loss = 0.0009\n",
      "Epoch 1881: loss = 0.0009\n",
      "Epoch 1891: loss = 0.0009\n",
      "Epoch 1901: loss = 0.0009\n",
      "Epoch 1911: loss = 0.0009\n",
      "Epoch 1921: loss = 0.0009\n",
      "Epoch 1931: loss = 0.0009\n",
      "Epoch 1941: loss = 0.0009\n",
      "Epoch 1951: loss = 0.0009\n",
      "Epoch 1961: loss = 0.0009\n",
      "Epoch 1971: loss = 0.0009\n",
      "Epoch 1981: loss = 0.0009\n",
      "Epoch 1991: loss = 0.0009\n",
      "[40, 47, 48, 49, 50, 8, 9, 16, 17, 24, 53, 54, 55, 56, 63, 46]\n"
     ]
    }
   ],
   "source": [
    "train_simple(num_epochs=2000, lr=0.003)\n",
    "\n",
    "# generate and check output\n",
    "print(generate_sequence([40], max_len=16, temperature=0.01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6a03ae9-0ef3-4a53-8df0-161aefe53d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 13, 20, 9, 71, 78, 71, 78, 71, 78]\n"
     ]
    }
   ],
   "source": [
    "print(generate_sequence([0,1,2,3,4,5,6], max_len=16, temperature=0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52203887-4275-400d-bbd1-2e014579c67b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
