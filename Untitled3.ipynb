{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d190c644-ae3e-4063-a342-853401bbfd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_size = 88        # number of possible notes\n",
    "seq_len = 24            # input length per example\n",
    "d_model = 128          # hidden size\n",
    "num_layers = 3           # transformer depth\n",
    "batch_size = 32        # how many examples per step\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cae10263-d59b-4f72-a4d4-9cdef9b234db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import optax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a91aa90-4a95-49c0-876e-f527a678d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.struct import dataclass\n",
    "from jax.tree_util import register_pytree_node_class\n",
    "\n",
    "@dataclass\n",
    "class TransformerParams:\n",
    "    W_q: jnp.ndarray\n",
    "    W_k: jnp.ndarray\n",
    "    W_v: jnp.ndarray\n",
    "    W_o: jnp.ndarray\n",
    "    W1: jnp.ndarray\n",
    "    b1: jnp.ndarray\n",
    "    W2: jnp.ndarray\n",
    "    b2: jnp.ndarray\n",
    "    gamma1: jnp.ndarray\n",
    "    beta1: jnp.ndarray\n",
    "    gamma2: jnp.ndarray\n",
    "    beta2: jnp.ndarray\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        children = (self.W_q, self.W_k, self.W_v, self.W_o,\n",
    "                    self.W1, self.b1, self.W2, self.b2,\n",
    "                    self.gamma1, self.beta1, self.gamma2, self.beta2)\n",
    "        aux = None\n",
    "        return children, aux\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux, children):\n",
    "        return cls(*children)\n",
    "\n",
    "@dataclass\n",
    "class StackedTransformerParams:\n",
    "    blocks: tuple[TransformerParams, ...]\n",
    "\n",
    "    def tree_flatten(self):\n",
    "        return (self.blocks,), None\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux, children):\n",
    "        return cls(*children)\n",
    "\n",
    "@dataclass\n",
    "class ModelParams:\n",
    "    embedding: jax.Array\n",
    "    transformer: StackedTransformerParams\n",
    "    W_out: jax.Array  # final projection to vocab\n",
    "\n",
    "\n",
    "def layernorm(x, gamma, beta, eps=1e-5):\n",
    "    mean = x.mean(-1, keepdims=True)\n",
    "    var = x.var(-1, keepdims=True)\n",
    "    norm = (x - mean) / jnp.sqrt(var + eps)\n",
    "    return gamma * norm + beta\n",
    "\n",
    "def attention(x, params):\n",
    "    Q = x @ params.W_q\n",
    "    K = x @ params.W_k\n",
    "    V = x @ params.W_v\n",
    "\n",
    "    scale = jnp.sqrt(x.shape[-1])\n",
    "    # scores = Q @ K.T / scale\n",
    "    scores = jnp.einsum('bij,bkj->bik', Q, K) / scale\n",
    "    mask = jnp.triu(jnp.ones_like(scores), 1) * -1e9\n",
    "    scores += mask\n",
    "    weights = jax.nn.softmax(scores, axis=-1)\n",
    "    attn = weights @ V\n",
    "    return attn @ params.W_o\n",
    "\n",
    "def feedforward(x, params):\n",
    "    h = jax.nn.relu(x @ params.W1 + params.b1)\n",
    "    return h @ params.W2 + params.b2\n",
    "\n",
    "def transformer_block(x, params):\n",
    "    attn_out = attention(x, params)\n",
    "    x1 = layernorm(x + attn_out, params.gamma1, params.beta1)\n",
    "    ff_out = feedforward(x1, params)\n",
    "    x2 = layernorm(x1 + ff_out, params.gamma2, params.beta2)\n",
    "    return x2\n",
    "\n",
    "def init_transformer_params(key, d_model):\n",
    "    k1, k2, k3 = jax.random.split(key, 3)\n",
    "    def norm_init(shape): return jax.random.normal(k1, shape) / jnp.sqrt(d_model)\n",
    "    def zero_init(shape): return jnp.zeros(shape)\n",
    "\n",
    "    return TransformerParams(\n",
    "        W_q = norm_init((d_model, d_model)),\n",
    "        W_k = norm_init((d_model, d_model)),\n",
    "        W_v = norm_init((d_model, d_model)),\n",
    "        W_o = norm_init((d_model, d_model)),\n",
    "\n",
    "        W1 = norm_init((d_model, 4 * d_model)),\n",
    "        b1 = zero_init((4 * d_model,)),\n",
    "        W2 = norm_init((4 * d_model, d_model)),\n",
    "        b2 = zero_init((d_model,)),\n",
    "\n",
    "        gamma1 = jnp.ones((d_model,)),\n",
    "        beta1 = jnp.zeros((d_model,)),\n",
    "        gamma2 = jnp.ones((d_model,)),\n",
    "        beta2 = jnp.zeros((d_model,))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afb2ad29-8765-4a6a-8128-c2a79c6dc36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding(key, vocab_size, d_model):\n",
    "    return jax.random.normal(key, (vocab_size, d_model)) * 0.01\n",
    "\n",
    "def embedding_fn(embedding_params, token_ids):\n",
    "    return embedding_params[token_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf4a3451-a1e2-4917-a69a-697b5d9bce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stacked_transformer_params(key, d_model, n_layers):\n",
    "    keys = jax.random.split(key, n_layers)\n",
    "    return StackedTransformerParams(\n",
    "        blocks=tuple(init_transformer_params(k, d_model) for k in keys)\n",
    "    )\n",
    "\n",
    "def transformer_block_batch(x, params: TransformerParams):\n",
    "    attn_out = attention(x, params)\n",
    "    x1 = layernorm(x + attn_out, params.gamma1, params.beta1)\n",
    "    ff_out = feedforward(x1, params)\n",
    "    x2 = layernorm(x1 + ff_out, params.gamma2, params.beta2)\n",
    "    return x2\n",
    "\n",
    "def stacked_forward(x, params: StackedTransformerParams):\n",
    "    for block_params in params.blocks:\n",
    "        x = transformer_block_batch(x, block_params)\n",
    "    return x\n",
    "\n",
    "def transformer_block_batch(x, params: TransformerParams):\n",
    "    attn_out = attention(x, params)\n",
    "    x1 = layernorm(x + attn_out, params.gamma1, params.beta1)\n",
    "    ff_out = feedforward(x1, params)\n",
    "    x2 = layernorm(x1 + ff_out, params.gamma2, params.beta2)\n",
    "    return x2\n",
    "\n",
    "def stacked_forward(x, params: StackedTransformerParams):\n",
    "    for block_params in params.blocks:\n",
    "        x = transformer_block_batch(x, block_params)\n",
    "    return x\n",
    "\n",
    "def cross_entropy_loss(logits, targets):\n",
    "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "    one_hot = jax.nn.one_hot(targets, logits.shape[-1])\n",
    "    return -jnp.sum(one_hot * log_probs) / targets.shape[0]\n",
    "\n",
    "def forward_and_loss(params: ModelParams, token_ids, targets):\n",
    "    x = embedding_fn(params.embedding, token_ids)  # (B, T, D)\n",
    "    x = stacked_forward(x, params.transformer)\n",
    "    logits = x @ params.W_out.T  # (B, T, vocab)\n",
    "    logits = logits.reshape(-1, logits.shape[-1])\n",
    "    targets = targets.reshape(-1)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ca6a8-40f0-4c5b-9f85-3fd5c13abc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, token_ids, targets):\n",
    "    def loss_fn(p):\n",
    "        return forward_and_loss(p, token_ids, targets)\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "key_embed, key_model, key_out = jax.random.split(key, 3)\n",
    "\n",
    "embedding_params = init_embedding(key_embed, vocab_size, d_model)\n",
    "transformer_params = init_stacked_transformer_params(key_model, d_model, num_layers)\n",
    "W_out = jax.random.normal(key_out, (vocab_size, d_model)) * 0.01\n",
    "\n",
    "params = ModelParams(\n",
    "    embedding=embedding_params,\n",
    "    transformer=transformer_params,\n",
    "    W_out=W_out\n",
    ")\n",
    "\n",
    "optimizer = optax.adamw(learning_rate=1e-4)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "inputs = jax.random.randint(key, (batch_size, seq_len), 0, vocab_size)\n",
    "targets = jax.random.randint(key, (batch_size, seq_len), 0, vocab_size)\n",
    "\n",
    "params, opt_state, loss = train_step(params, opt_state, inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd609026-976e-49b0-a853-d80ad32ad63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(logits, target):\n",
    "    probs = softmax(logits)\n",
    "    loss = -np.log(probs[target] + 1e-9)\n",
    "    dlogits = probs\n",
    "    dlogits[target] -= 1\n",
    "    return loss, dlogits\n",
    "\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.zeros((seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return pos_encoding\n",
    "\n",
    "positional_encoding = get_positional_encoding(seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c272f002-541a-47e8-a11f-d9d3c8257d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layernorm_backward(dout, x, mean, var, gamma, eps=1e-5):\n",
    "    N, D = x.shape\n",
    "    x_mu = x - mean\n",
    "    std_inv = 1. / np.sqrt(var + eps)\n",
    "\n",
    "    dnorm = dout * gamma\n",
    "    dvar = np.sum(dnorm * x_mu, axis=-1, keepdims=True) * -0.5 * std_inv**3\n",
    "    dmean = np.sum(dnorm * -std_inv, axis=-1, keepdims=True) + dvar * np.mean(-2.0 * x_mu, axis=-1, keepdims=True)\n",
    "\n",
    "    dx = dnorm * std_inv + dvar * 2 * x_mu / D + dmean / D\n",
    "    dgamma = np.sum(dout * (x_mu * std_inv), axis=0)\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    return dx, dgamma, dbeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56acb7-f841-4be2-b58b-58a3f70c851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformerblock class\n",
    "class TransformerBlock:\n",
    "    def __init__(self, d_model):\n",
    "        self.d_model = d_model\n",
    "        self.gamma1 = np.ones((self.d_model,))\n",
    "        self.beta1 = np.zeros((self.d_model,))\n",
    "        self.gamma2 = np.ones((self.d_model,))\n",
    "        self.beta2 = np.zeros((self.d_model,))\n",
    "\n",
    "        self.W_q = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_o = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "\n",
    "        self.W1 = np.random.randn(d_model, 4*d_model) / np.sqrt(d_model)\n",
    "        self.b1 = np.zeros((4*d_model,))\n",
    "        self.W2 = np.random.randn(4*d_model, d_model) / np.sqrt(4*d_model)\n",
    "        self.b2 = np.zeros((d_model,))\n",
    "\n",
    "        self.cache = {}  # to store intermediate states for backprop\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache['x'] = x.copy()\n",
    "    \n",
    "        # self-attention\n",
    "        Q = x @ self.W_q\n",
    "        K = x @ self.W_k\n",
    "        V = x @ self.W_v\n",
    "        scale = np.sqrt(self.d_model)\n",
    "    \n",
    "        scores = Q @ K.T / scale\n",
    "        mask = np.triu(np.ones_like(scores), 1) * -1e9\n",
    "        scores += mask\n",
    "    \n",
    "        weights = softmax(scores)\n",
    "        self.cache.update({'Q': Q, 'K': K, 'V': V, 'weights': weights})\n",
    "    \n",
    "        attn = weights @ V\n",
    "        attn_out = attn @ self.W_o\n",
    "        x_attn = x + attn_out\n",
    "    \n",
    "        # layernorm 1\n",
    "        mean1 = x_attn.mean(axis=-1, keepdims=True)\n",
    "        var1 = x_attn.var(axis=-1, keepdims=True)\n",
    "        norm1 = (x_attn - mean1) / np.sqrt(var1 + 1e-5)\n",
    "        x2 = self.gamma1 * norm1 + self.beta1\n",
    "        self.cache.update({'ln1_mean': mean1, 'ln1_var': var1, 'x2': x2})\n",
    "        \n",
    "    \n",
    "        # feedforward\n",
    "        h = np.maximum(0, x2 @ self.W1 + self.b1)\n",
    "        ff_out = h @ self.W2 + self.b2\n",
    "        self.cache['ff_h'] = h\n",
    "    \n",
    "        x_ffn = x2 + ff_out\n",
    "    \n",
    "        # layernorm 2\n",
    "        mean2 = x_ffn.mean(axis=-1, keepdims=True)\n",
    "        var2 = x_ffn.var(axis=-1, keepdims=True)\n",
    "        norm2 = (x_ffn - mean2) / np.sqrt(var2 + 1e-5)\n",
    "        out = self.gamma2 * norm2 + self.beta2\n",
    "        self.cache.update({'ln2_mean': mean2, 'ln2_var': var2, 'out': out})\n",
    "    \n",
    "        return out\n",
    "    \n",
    "        \n",
    "    def layernorm(self, x, eps=1e-5):\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        var = x.var(axis=-1, keepdims=True)\n",
    "        norm = (x - mean) / np.sqrt(var + eps)\n",
    "        self.cache['ln_mean'], self.cache['ln_var'] = mean, var\n",
    "        self.cache['ln_input'] = x.copy()\n",
    "        return norm  # gamma/beta omitted for simplicity\n",
    "\n",
    "    def layernorm_backward(dout, x, mean, var, gamma, eps=1e-5):\n",
    "        N, D = x.shape\n",
    "        x_mu = x - mean\n",
    "        std_inv = 1. / np.sqrt(var + eps)\n",
    "    \n",
    "        dnorm = dout * gamma\n",
    "        dvar = np.sum(dnorm * x_mu, axis=-1, keepdims=True) * -0.5 * std_inv**3\n",
    "        dmean = np.sum(dnorm * -std_inv, axis=-1, keepdims=True) + dvar * np.mean(-2.0 * x_mu, axis=-1, keepdims=True)\n",
    "    \n",
    "        dx = dnorm * std_inv + dvar * 2 * x_mu / D + dmean / D\n",
    "        dgamma = np.sum(dout * (x_mu * std_inv), axis=0)\n",
    "        dbeta = np.sum(dout, axis=0)\n",
    "        return dx, dgamma, dbeta\n",
    "\n",
    "    def backward(self, dout):\n",
    "        grads = {}\n",
    "    \n",
    "        # -------------------------\n",
    "        # layernorm 2 backward\n",
    "        # -------------------------\n",
    "        x2 = self.cache['x2']\n",
    "        out = self.cache['out']\n",
    "        mean2, var2 = self.cache['ln2_mean'], self.cache['ln2_var']\n",
    "        gamma2, beta2 = self.gamma2, self.beta2\n",
    "    \n",
    "        dnorm2, dgamma2, dbeta2 = layernorm_backward(\n",
    "            dout, out, mean2, var2, gamma2\n",
    "        )\n",
    "        grads['gamma2'], grads['beta2'] = dgamma2, dbeta2\n",
    "    \n",
    "        # -------------------------\n",
    "        # residual from FFN\n",
    "        # -------------------------\n",
    "        dff_out = dnorm2\n",
    "        dff_in, dW1, db1, dW2, db2 = self.feedforward_backward(dff_out)\n",
    "        grads['W1'], grads['b1'] = dW1, db1\n",
    "        grads['W2'], grads['b2'] = dW2, db2\n",
    "    \n",
    "        # -------------------------\n",
    "        # layernorm 1 backward\n",
    "        # -------------------------\n",
    "        x = self.cache['x']\n",
    "        mean1, var1 = self.cache['ln1_mean'], self.cache['ln1_var']\n",
    "        gamma1, beta1 = self.gamma1, self.beta1\n",
    "    \n",
    "        dnorm1 = dff_in + dnorm2  # residual connection\n",
    "        dln1, dgamma1, dbeta1 = layernorm_backward(dnorm1, x, mean1, var1, gamma1)\n",
    "        grads['gamma1'], grads['beta1'] = dgamma1, dbeta1\n",
    "    \n",
    "        # -------------------------\n",
    "        # attention backward\n",
    "        # -------------------------\n",
    "        dx_attn, dW_q, dW_k, dW_v, dW_o = self.attention_backward(dln1)\n",
    "        grads['W_q'], grads['W_k'], grads['W_v'], grads['W_o'] = dW_q, dW_k, dW_v, dW_o\n",
    "    \n",
    "        # -------------------------\n",
    "        # input gradient\n",
    "        # -------------------------\n",
    "        dx = dx_attn + dln1  # residual connection from attention\n",
    "    \n",
    "        return dx, grads\n",
    "\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        h = np.maximum(0, x @ self.W1 + self.b1)\n",
    "        self.cache['ff_h'] = h\n",
    "        return h @ self.W2 + self.b2\n",
    "\n",
    "    def feedforward_backward(self, dout):\n",
    "        h = self.cache['ff_h']\n",
    "        x2 = self.cache['x2']\n",
    "    \n",
    "        dW2 = h.T @ dout\n",
    "        db2 = np.sum(dout, axis=0)\n",
    "        dh = dout @ self.W2.T\n",
    "        dh[h <= 0] = 0  # ReLU backprop\n",
    "    \n",
    "        dW1 = x2.T @ dh\n",
    "        db1 = np.sum(dh, axis=0)\n",
    "        dx2 = dh @ self.W1.T\n",
    "    \n",
    "        return dx2, dW1, db1, dW2, db2\n",
    "\n",
    "    def attention_backward(self, dout):\n",
    "        \"\"\"\n",
    "        dout: ∂L/∂(attn_out @ W_o), shape (T, d_model)\n",
    "        returns:\n",
    "            dx: ∂L/∂x\n",
    "            dW_q, dW_k, dW_v, dW_o\n",
    "        \"\"\"\n",
    "        Q, K, V = self.cache['Q'], self.cache['K'], self.cache['V']\n",
    "        weights = self.cache['weights']\n",
    "        x = self.cache['x']\n",
    "        T, D = x.shape\n",
    "        scale = np.sqrt(D)\n",
    "    \n",
    "        # ---- W_o backward ----\n",
    "        attn = weights @ V        # (T, D)\n",
    "        dW_o = attn.T @ dout      # (D, D)\n",
    "        dattn = dout @ self.W_o.T # (T, D)\n",
    "    \n",
    "        # ---- softmax(V) backward ----\n",
    "        dweights = dattn @ V.T    # (T, T)\n",
    "        dV = weights.T @ dattn    # (T, D)\n",
    "    \n",
    "        # ---- softmax backward (Jacobian vector trick) ----\n",
    "        dscores = np.zeros_like(dweights)\n",
    "        for t in range(T):\n",
    "            w = weights[t]                             # (T,)\n",
    "            dw = dweights[t]                           # (T,)\n",
    "            jac = np.diag(w) - np.outer(w, w)          # softmax Jacobian\n",
    "            dscores[t] = jac @ dw                      # (T,)\n",
    "    \n",
    "        # ---- scores = Q @ K.T / scale ----\n",
    "        dQ = dscores @ K / scale                       # (T, D)\n",
    "        dK = dscores.T @ Q / scale                     # (T, D)\n",
    "    \n",
    "        # ---- Q = x @ W_q, K = x @ W_k, V = x @ W_v ----\n",
    "        dW_q = x.T @ dQ\n",
    "        dW_k = x.T @ dK\n",
    "        dW_v = x.T @ dV\n",
    "    \n",
    "        dx_q = dQ @ self.W_q.T\n",
    "        dx_k = dK @ self.W_k.T\n",
    "        dx_v = dV @ self.W_v.T\n",
    "    \n",
    "        dx = dx_q + dx_k + dx_v  # aggregate residuals from 3 projections\n",
    "    \n",
    "        return dx, dW_q, dW_k, dW_v, dW_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf49982a-ddd8-40f8-96c5-febe4ba402c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def make_training_data():\n",
    "    return [list(range(i, i + seq_len)) for i in range(vocab_size - seq_len + 1)]\n",
    "\n",
    "def make_checkpoint(transformer, token_embedding, W_out):\n",
    "    return {\n",
    "        \"transformer\": copy.deepcopy(transformer),\n",
    "        \"token_embedding\": np.copy(token_embedding),\n",
    "        \"W_out\": np.copy(W_out)\n",
    "    }\n",
    "\n",
    "def restore_checkpoint(checkpoint):\n",
    "    transformer = copy.deepcopy(checkpoint[\"transformer\"])\n",
    "    token_embedding = np.copy(checkpoint[\"token_embedding\"])\n",
    "    W_out = np.copy(checkpoint[\"W_out\"])\n",
    "    return transformer, token_embedding, W_out\n",
    "\n",
    "\n",
    "def train(transformer, token_embedding, W_out, data, epochs=100):\n",
    "    scheduler = StablePlateauScheduler()\n",
    "    checkpoint = make_checkpoint(transformer, token_embedding, W_out)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        lr = scheduler.get_lr()\n",
    "        for seq in data:\n",
    "\n",
    "\n",
    "            input_seq = seq[:-1]    # [0..6]\n",
    "            target_seq = seq[1:]    # [1..7]\n",
    "\n",
    "            x = token_embedding[input_seq]\n",
    "            x += positional_encoding[:len(input_seq)]\n",
    "            out = transformer.forward(x)   # (T, d_model)\n",
    "\n",
    "            d_out = np.zeros_like(out)\n",
    "            loss = 0\n",
    "\n",
    "            for t in range(len(target_seq)):\n",
    "                logits = out[t] @ W_out  # (vocab_size,)\n",
    "                step_loss, dlogits = cross_entropy_loss(logits, target_seq[t])\n",
    "                loss += step_loss\n",
    "\n",
    "                # backprop into out[t]\n",
    "                d_out[t] = dlogits @ W_out.T\n",
    "\n",
    "                # update W_out\n",
    "                W_out -= lr * np.outer(out[t], dlogits)\n",
    "\n",
    "            # total loss for sequence\n",
    "            total_loss += loss / len(target_seq)\n",
    "\n",
    "            # backward through transformer\n",
    "            dx, grads = transformer.backward(d_out)\n",
    "\n",
    "            for name, grad in grads.items():\n",
    "                param = getattr(transformer, name)\n",
    "                setattr(transformer, name, param - lr * grad)\n",
    "\n",
    "            for i, idx in enumerate(input_seq):\n",
    "                token_embedding[idx] -= lr * dx[i]\n",
    "\n",
    "        avg_loss = total_loss / len(data)\n",
    "    \n",
    "        # check if this is best run so far\n",
    "        if avg_loss < best_loss - 1e-4:\n",
    "            best_loss = avg_loss\n",
    "            checkpoint = make_checkpoint(transformer, token_embedding, W_out)\n",
    "    \n",
    "        # update scheduler, restore checkpoint if decayed\n",
    "        decayed = scheduler.update(avg_loss)\n",
    "        if decayed:\n",
    "            transformer, token_embedding, W_out = restore_checkpoint(checkpoint)\n",
    "\n",
    "        print(f\"epoch {epoch+1:03}  loss = {total_loss/len(data):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be37b7c-600d-4a41-807a-ed7ef39d5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_probs(probs, temperature=1.0):\n",
    "    if temperature != 1.0:\n",
    "        probs = np.log(probs + 1e-9) / temperature\n",
    "        probs = np.exp(probs - np.max(probs))\n",
    "        probs /= np.sum(probs)\n",
    "    return int(np.random.choice(len(probs), p=probs))\n",
    "\n",
    "def generate_sequence(transformer, token_embedding, W_out, start_seq, max_len=16, temperature=1.0):\n",
    "    seq = list(start_seq)\n",
    "    for _ in range(max_len - len(seq)):\n",
    "        input_seq = seq[-seq_len:]\n",
    "        x = token_embedding[input_seq]\n",
    "        x += positional_encoding[:len(input_seq)]\n",
    "        out = transformer.forward(x)\n",
    "        logits = out[-1] @ W_out\n",
    "        probs = softmax(logits)\n",
    "        next_note = sample_from_probs(probs, temperature)\n",
    "        seq.append(next_note)\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0985b4be-550f-4b36-94ea-9c359aba0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(transformer, token_embedding, W_out, start_seq, max_len=16, beam_width=3, temperature=1.0, verbose=False):\n",
    "    beams = [(start_seq, 0.0)]  # (sequence, total log-prob)\n",
    "\n",
    "    for step in range(max_len - len(start_seq)):\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, logp in beams:\n",
    "            input_seq = seq[-seq_len:]\n",
    "            x = token_embedding[input_seq]\n",
    "            x += positional_encoding[:len(input_seq)]\n",
    "            out = transformer.forward(x)\n",
    "\n",
    "            logits = out[-1] @ W_out\n",
    "            logits = logits / temperature  # apply temperature\n",
    "            probs = softmax(logits)\n",
    "\n",
    "            for token in range(vocab_size):\n",
    "                new_seq = seq + [token]\n",
    "                new_logp = logp + np.log(probs[token] + 1e-9)\n",
    "                avg_logp = new_logp / len(new_seq)\n",
    "                all_candidates.append((new_seq, avg_logp))\n",
    "\n",
    "        beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n-- Step {step+1} --\")\n",
    "            for i, (bseq, bscore) in enumerate(beams):\n",
    "                print(f\"Beam {i+1}: {bseq}  avg_logp = {bscore:.3f}\")\n",
    "\n",
    "    return beams[0][0]  # best sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66fb073f-e831-4d0e-a13e-6b9718ba5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_data_from_filename(filename, seq_len=8, vocab_size=88, filter_out_of_range=True):\n",
    "    \"\"\"\n",
    "    Reads a file containing space-separated integers, slices it into (seq_len + 1)-length windows,\n",
    "    and returns training sequences for next-token prediction.\n",
    "\n",
    "    Each sequence of length seq_len+1 is split into:\n",
    "        input  = seq[:-1]\n",
    "        target = seq[1:]\n",
    "\n",
    "    Args:\n",
    "        filename (str): path to the file containing space-separated note numbers\n",
    "        seq_len (int): number of input tokens per training example (default: 8)\n",
    "        vocab_size (int): maximum allowed token value (default: 88 for piano keys)\n",
    "        filter_out_of_range (bool): whether to drop values outside [0, vocab_size-1]\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: list of (seq_len + 1)-long sequences for training\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    tokens = list(map(int, text.strip().split()))\n",
    "\n",
    "    if filter_out_of_range:\n",
    "        tokens = [t for t in tokens if 0 <= t < vocab_size]\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(tokens) - seq_len):\n",
    "        seq = tokens[i:i + seq_len + 1]\n",
    "        data.append(seq)\n",
    "\n",
    "    return data\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def make_training_data_from_directory(path=\"./data/*.txt\", seq_len=8, vocab_size=88, filter_out_of_range=True):\n",
    "    \"\"\"\n",
    "    Reads all .txt files from the given directory glob, where each file contains\n",
    "    space-separated note values. Generates training sequences independently per file.\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]] – list of (seq_len + 1)-long sequences for training\n",
    "    \"\"\"\n",
    "    all_sequences = []\n",
    "\n",
    "    for filepath in glob.glob(path):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            text = f.read()\n",
    "        tokens = list(map(int, text.strip().split()))\n",
    "        if filter_out_of_range:\n",
    "            tokens = [t for t in tokens if 0 <= t < vocab_size]\n",
    "        for i in range(len(tokens) - seq_len):\n",
    "            seq = tokens[i : i + seq_len + 1]\n",
    "            all_sequences.append(seq)\n",
    "\n",
    "    return all_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f30e7d1e-d56d-4068-ba3f-053c07961ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "def load_training_data_from_directory(path: str, seq_len: int) -> list[np.ndarray]:\n",
    "    sequences = []\n",
    "    for filename in glob.glob(f\"{path}/*.txt\"):\n",
    "        sequences.extend(make_training_data_from_filename(filename, seq_len))\n",
    "    return sequences\n",
    "\n",
    "def batch_generator(sequences: list[np.ndarray], batch_size: int):\n",
    "    while True:\n",
    "        batch = random.sample(sequences, batch_size)\n",
    "        batch = np.stack(batch, axis=0)  # (B, T)\n",
    "        yield batch[:, :-1], batch[:, 1:]  # inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "eb915879-4329-4cd8-be7b-31fe7b244e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss = 0.2521265745162964\n",
      "step 100: loss = 0.20547634363174438\n",
      "step 200: loss = 0.2230464071035385\n",
      "step 300: loss = 0.2069634646177292\n",
      "step 400: loss = 0.20980314910411835\n",
      "step 500: loss = 0.21924088895320892\n",
      "step 600: loss = 0.2374354898929596\n",
      "step 700: loss = 0.21206580102443695\n",
      "step 800: loss = 0.21270889043807983\n",
      "step 900: loss = 0.20445606112480164\n",
      "step 1000: loss = 0.21432623267173767\n",
      "step 1100: loss = 0.20468923449516296\n",
      "step 1200: loss = 0.2119407057762146\n",
      "step 1300: loss = 0.20189213752746582\n",
      "step 1400: loss = 0.21416395902633667\n",
      "step 1500: loss = 0.19687935709953308\n",
      "step 1600: loss = 0.20304489135742188\n",
      "step 1700: loss = 0.2048814594745636\n",
      "step 1800: loss = 0.2238340675830841\n",
      "step 1900: loss = 0.2050248086452484\n",
      "step 2000: loss = 0.202180415391922\n",
      "step 2100: loss = 0.19321611523628235\n",
      "step 2200: loss = 0.18292373418807983\n",
      "step 2300: loss = 0.19748644530773163\n",
      "step 2400: loss = 0.19334951043128967\n",
      "step 2500: loss = 0.20502787828445435\n",
      "step 2600: loss = 0.20585255324840546\n",
      "step 2700: loss = 0.19583867490291595\n",
      "step 2800: loss = 0.1887327879667282\n",
      "step 2900: loss = 0.21290439367294312\n",
      "step 3000: loss = 0.19340775907039642\n",
      "step 3100: loss = 0.22184012830257416\n",
      "step 3200: loss = 0.19720064103603363\n",
      "step 3300: loss = 0.2179429531097412\n",
      "step 3400: loss = 0.20357805490493774\n",
      "step 3500: loss = 0.20166605710983276\n",
      "step 3600: loss = 0.1935720443725586\n",
      "step 3700: loss = 0.18415474891662598\n",
      "step 3800: loss = 0.21098266541957855\n",
      "step 3900: loss = 0.19303232431411743\n",
      "step 4000: loss = 0.1974881887435913\n",
      "step 4100: loss = 0.20059221982955933\n",
      "step 4200: loss = 0.191116064786911\n",
      "step 4300: loss = 0.20688235759735107\n",
      "step 4400: loss = 0.19840297102928162\n",
      "step 4500: loss = 0.20689940452575684\n",
      "step 4600: loss = 0.2017771303653717\n",
      "step 4700: loss = 0.21591567993164062\n",
      "step 4800: loss = 0.1888241022825241\n",
      "step 4900: loss = 0.2093099057674408\n",
      "step 5000: loss = 0.1832059621810913\n",
      "step 5100: loss = 0.1970694363117218\n",
      "step 5200: loss = 0.2105255126953125\n",
      "step 5300: loss = 0.1955406665802002\n",
      "step 5400: loss = 0.20985856652259827\n",
      "step 5500: loss = 0.19544671475887299\n",
      "step 5600: loss = 0.19360767304897308\n",
      "step 5700: loss = 0.1870843768119812\n",
      "step 5800: loss = 0.2155126929283142\n",
      "step 5900: loss = 0.1835850179195404\n",
      "step 6000: loss = 0.18449068069458008\n",
      "step 6100: loss = 0.19647222757339478\n",
      "step 6200: loss = 0.1918528527021408\n",
      "step 6300: loss = 0.20270347595214844\n",
      "step 6400: loss = 0.18623042106628418\n",
      "step 6500: loss = 0.20581242442131042\n",
      "step 6600: loss = 0.18214087188243866\n",
      "step 6700: loss = 0.1992926448583603\n",
      "step 6800: loss = 0.2121601700782776\n",
      "step 6900: loss = 0.18998983502388\n",
      "step 7000: loss = 0.1948024332523346\n",
      "step 7100: loss = 0.19064460694789886\n",
      "step 7200: loss = 0.20105759799480438\n",
      "step 7300: loss = 0.2005957067012787\n",
      "step 7400: loss = 0.18561312556266785\n",
      "step 7500: loss = 0.19989800453186035\n",
      "step 7600: loss = 0.18893396854400635\n",
      "step 7700: loss = 0.1836520880460739\n",
      "step 7800: loss = 0.2049962729215622\n",
      "step 7900: loss = 0.19643078744411469\n",
      "step 8000: loss = 0.1847183108329773\n",
      "step 8100: loss = 0.20452314615249634\n",
      "step 8200: loss = 0.19064795970916748\n",
      "step 8300: loss = 0.18034248054027557\n",
      "step 8400: loss = 0.18139301240444183\n",
      "step 8500: loss = 0.19922992587089539\n",
      "step 8600: loss = 0.16760581731796265\n",
      "step 8700: loss = 0.18025819957256317\n",
      "step 8800: loss = 0.23854777216911316\n",
      "step 8900: loss = 0.18996202945709229\n",
      "step 9000: loss = 0.19826708734035492\n",
      "step 9100: loss = 0.21045050024986267\n",
      "step 9200: loss = 0.1849132478237152\n",
      "step 9300: loss = 0.17849251627922058\n",
      "step 9400: loss = 0.20269227027893066\n",
      "step 9500: loss = 0.18709522485733032\n",
      "step 9600: loss = 0.18199467658996582\n",
      "step 9700: loss = 0.2120775580406189\n",
      "step 9800: loss = 0.19028666615486145\n",
      "step 9900: loss = 0.19800904393196106\n"
     ]
    }
   ],
   "source": [
    "your_sequences = load_training_data_from_directory('./data', seq_len)\n",
    "train_gen = batch_generator(your_sequences, batch_size)\n",
    "num_steps = 10000\n",
    "\n",
    "for step in range(num_steps):\n",
    "    inputs, targets = next(train_gen)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    noise = jax.random.normal(subkey, inputs.shape + (d_model,))\n",
    "    params, opt_state, loss = train_step(params, opt_state, inputs, targets)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step {step}: loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7faf061d-8f1b-4e5f-a996-f247d841bd1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m transformer = \u001b[43mTransformerBlock\u001b[49m(d_model)\n\u001b[32m      2\u001b[39m W_out = np.random.randn(d_model, vocab_size) / np.sqrt(d_model)\n\u001b[32m      3\u001b[39m token_embedding = np.random.randn(vocab_size, d_model) * \u001b[32m0.01\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'TransformerBlock' is not defined"
     ]
    }
   ],
   "source": [
    "transformer = TransformerBlock(d_model)\n",
    "W_out = np.random.randn(d_model, vocab_size) / np.sqrt(d_model)\n",
    "token_embedding = np.random.randn(vocab_size, d_model) * 0.01\n",
    "data = make_training_data_from_directory(\"./data/*.txt\", seq_len=seq_len)\n",
    "# epochs = 600\n",
    "train(transformer, token_embedding, W_out, data, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d7f436-3ac8-425e-ab8e-644043c986ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class StablePlateauScheduler:\n",
    "    def __init__(self, lr=1e-2, decay_factor=0.9, patience=10, min_lr=1e-5, window_size=5, tolerance=1e-4):\n",
    "        self.lr = lr\n",
    "        self.decay_factor = decay_factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.window_size = window_size\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "        self.loss_window = deque(maxlen=window_size)\n",
    "        self.best_avg = float('inf')\n",
    "        self.epochs_since_improvement = 0\n",
    "        self.decay_enabled = True\n",
    "\n",
    "    def update(self, loss):\n",
    "        self.loss_window.append(loss)\n",
    "\n",
    "        if len(self.loss_window) < self.window_size:\n",
    "            return  # wait until window is full\n",
    "\n",
    "        current_avg = sum(self.loss_window) / self.window_size\n",
    "\n",
    "        if current_avg < self.best_avg - self.tolerance:\n",
    "            self.best_avg = current_avg\n",
    "            self.epochs_since_improvement = 0\n",
    "        else:\n",
    "            self.epochs_since_improvement += 1\n",
    "\n",
    "            if self.epochs_since_improvement >= self.patience and self.decay_enabled:\n",
    "                old_lr = self.lr\n",
    "                new_lr = max(self.lr * self.decay_factor, self.min_lr)\n",
    "                if new_lr < self.lr:\n",
    "                    self.lr = new_lr\n",
    "                    print(f\"↘ learning rate dropped from {old_lr:.2e} to {self.lr:.2e}\")\n",
    "                else:\n",
    "                    self.decay_enabled = False\n",
    "                self.epochs_since_improvement = 0\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d07851-cd62-4d6b-a192-e7130ab795d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = beam_search(transformer, token_embedding, W_out, [60, 63, 65, 67, 65, 63], max_len=24, beam_width=5, temperature=0.01)\n",
    "print(\"Generated:\", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cd0912c6-6dfd-4b14-8690-18361a0122c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fluidsynth: warning: No preset found on channel 9 [bank=128 prog=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluidSynth runtime version 2.3.4\n",
      "Copyright (C) 2000-2023 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of Creative Technology Ltd.\n",
      "\n",
      "Rendering audio to file 'yaman.wav'..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffplay version 6.1.1-3ubuntu5 Copyright (c) 2003-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, wav, from 'mixed.wav':   0KB vq=    0KB sq=    0B f=0/0   \n",
      "  Duration: 00:00:14.40, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "  14.28 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B f=0/0   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  14.34 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B f=0/0   "
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ffplay', '-nodisp', '-autoexit', 'mixed.wav'], returncode=0)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mido\n",
    "import os\n",
    "import subprocess\n",
    "from mido import Message, MidiFile, MidiTrack, MetaMessage\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "\n",
    "BPM=200\n",
    "\n",
    "def notes_to_midi_file(notes, filename='out.mid', bpm=BPM, velocity=64, duration=480, tempo=500000):\n",
    "    mid = MidiFile()\n",
    "    track = MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "\n",
    "    tick_per_beat = mid.ticks_per_beat\n",
    "    tick = int(tick_per_beat / 2)\n",
    "\n",
    "    # set tempo (default 120bpm)\n",
    "    tempo = int(60_000_000 / bpm)\n",
    "    track.append(MetaMessage('set_tempo', tempo=tempo, time=0))\n",
    "\n",
    "    for note in notes:\n",
    "        if note is None:\n",
    "            # silent: rest for one note duration\n",
    "            track.append(Message(\"note_off\", note=0, velocity=0, time=tick))\n",
    "        else:\n",
    "            pitch = note\n",
    "            track.append(Message(\"note_on\", note=pitch, velocity=100, time=0))\n",
    "            track.append(Message(\"note_off\", note=pitch, velocity=100, time=tick))\n",
    "\n",
    "    mid.save(filename)\n",
    "\n",
    "midi_path = 'yaman.mid'\n",
    "wav_path = 'yaman.wav'\n",
    "# notes_to_midi_file(generated, midi_path)\n",
    "soundfont_path = os.path.expanduser(\"~/Downloads/harmonium-samples-20250608T203259Z-1-001/harmonium-samples/trimmed/trimmed/harmonium.sf2\")\n",
    "taal_path = os.path.expanduser(\"~/Documents/music/ektaal_200bpm_csharp.wav\")\n",
    "def render_midi_to_wav(midi_path=midi_path, wav_path=wav_path, sf2_path=soundfont_path):\n",
    "    subprocess.run([\n",
    "        \"fluidsynth\",\n",
    "        \"-g\", \"2.0\",       # 🔊 boost gain\n",
    "        \"-ni\", sf2_path,\n",
    "        midi_path,\n",
    "        \"-F\", wav_path,\n",
    "        \"-r\", \"48000\"      # 🧩 match taal.wav sample rate\n",
    "    ])\n",
    "render_midi_to_wav()\n",
    "\n",
    "def mix_audio(taal_path, taan_path, out_path):\n",
    "    taal = AudioSegment.from_wav(taal_path)\n",
    "    taan = AudioSegment.from_wav(taan_path)\n",
    "\n",
    "    # match volume (boost taan if needed)\n",
    "    taan = taan - 2 # 🔊 increase volume by 6dB\n",
    "\n",
    "    # optional trim/pad to align lengths\n",
    "    taan = taan[:len(taal)]\n",
    "\n",
    "    combined = taal.overlay(taan)\n",
    "    combined.export(out_path, format=\"wav\")\n",
    "\n",
    "mix_audio(taal_path, 'yaman.wav', 'mixed.wav')\n",
    "subprocess.run([\"ffplay\", \"-nodisp\", \"-autoexit\", 'mixed.wav'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a6f9e635-a75a-490d-ab9e-2048dba18286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fluidsynth: warning: No preset found on channel 9 [bank=128 prog=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluidSynth runtime version 2.3.4\n",
      "Copyright (C) 2000-2023 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of Creative Technology Ltd.\n",
      "\n",
      "Rendering audio to file 'yaman.wav'..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffplay version 6.1.1-3ubuntu5 Copyright (c) 2003-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, wav, from 'mixed.wav':   0KB vq=    0KB sq=    0B f=0/0   \n",
      "  Duration: 00:01:42.80, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      " 102.75 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B f=0/0   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ffplay', '-nodisp', '-autoexit', 'mixed.wav'], returncode=0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def sample_autoregressively(params: ModelParams, start_tokens, max_len=32, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    start_tokens: (T,) array of initial token ids\n",
    "    returns: (T + max_len,) array of generated token ids\n",
    "    \"\"\"\n",
    "    generated = list(start_tokens)\n",
    "    x = jnp.array(generated)[None, :]  # shape (1, T)\n",
    "\n",
    "    for i in range(max_len):\n",
    "        x_emb = embedding_fn(params.embedding, x)  # (1, T, D)\n",
    "        x_out = stacked_forward(x_emb, params.transformer)  # (1, T, D)\n",
    "        logits = x_out[:, -1, :] @ params.W_out.T  # (1, vocab)\n",
    "        logits = logits / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits = jnp.sort(logits, axis=-1)[:, -top_k]\n",
    "            logits = jnp.where(logits < top_logits[:, None], -1e10, logits)\n",
    "\n",
    "        probs = jax.nn.softmax(logits, axis=-1)\n",
    "        next_token = jax.random.categorical(jax.random.PRNGKey(np.random.randint(1e6)), logits).item()\n",
    "\n",
    "        generated.append(next_token)\n",
    "        x = jnp.array(generated)[None, :]\n",
    "\n",
    "        # early stopping\n",
    "        if ((i+1)+len(start_tokens)) % 24 == 0 and next_token in [61, 73]:\n",
    "            break\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "def generate_looped_taan_sequence(n_taans, taan_generator, pad_token=None):\n",
    "    \"\"\"\n",
    "    Generate a sequence of notes with taans and rests in between.\n",
    "    Each taan: 24 notes\n",
    "    Each rest: 24 steps (None)\n",
    "    \"\"\"\n",
    "    all_notes = []\n",
    "    for _ in range(n_taans):\n",
    "        taan = taan_generator()\n",
    "        assert len(taan) % 24 == 0, f\"Taan generator must return multiple of 24 notes: {len(taan)} notes\"\n",
    "        all_notes.extend(taan)\n",
    "        all_notes.extend([pad_token] * 24)\n",
    "    return all_notes\n",
    "\n",
    "def one_taan():\n",
    "    # return constrained_beam_search(params, 60, 61, 8, 23, 1)\n",
    "    for _ in range(10):\n",
    "        taan = sample_autoregressively(params, [random.choice([60,68,73])], 71, 1, top_k=None)\n",
    "        if taan[-1] in [61, 73]:\n",
    "            return taan\n",
    "    return taan\n",
    "    # return generate_sequence(transformer, token_embedding, W_out, [60], max_len=48, temperature=0.9)\n",
    "\n",
    "n_taans = 10\n",
    "note_sequence = generate_looped_taan_sequence(n_taans, one_taan)\n",
    "\n",
    "notes_to_midi_file(note_sequence, midi_path, bpm=BPM)\n",
    "render_midi_to_wav()\n",
    "\n",
    "# 🔁 overlay enough taal cycles (we handle it by looping audio)\n",
    "def loop_audio_to_match(reference_audio, loop_audio):\n",
    "    looped = loop_audio * (len(reference_audio) // len(loop_audio) + 1)\n",
    "    return looped[:len(reference_audio)]\n",
    "\n",
    "def mix_audio_looped(taal_path, taan_path, out_path):\n",
    "    taal = AudioSegment.from_wav(taal_path)\n",
    "    taan = AudioSegment.from_wav(taan_path)\n",
    "\n",
    "    taal_looped = loop_audio_to_match(taan, taal)\n",
    "    mixed = taal_looped.overlay(taan)\n",
    "    mixed.export(out_path, format=\"wav\")\n",
    "\n",
    "mix_audio_looped(taal_path, 'yaman.wav', 'mixed.wav')\n",
    "subprocess.run([\"ffplay\", \"-nodisp\", \"-autoexit\", 'mixed.wav'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "636fe6b4-234b-49c5-8729-2fd5f08c1feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import pickle\n",
    "\n",
    "# save\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    bytes_output = flax.serialization.to_bytes(params)\n",
    "    pickle.dump(bytes_output, f)\n",
    "\n",
    "# load\n",
    "# with open(\"model.pkl\", \"rb\") as f:\n",
    "#     bytes_input = pickle.load(f)\n",
    "#     params = flax.serialization.from_bytes(params, bytes_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d21c49-fcac-4b09-b6ed-b4c64e4f7c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
